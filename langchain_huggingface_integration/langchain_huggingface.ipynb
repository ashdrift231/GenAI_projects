{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Install required libraries for Langchain, Hugging Face models, and related dependencies.\n",
        "langchain: Core Langchain library for building LLM applications.\n",
        "\n",
        "langchain-huggingface: Integration for Hugging Face models within Langchain.\n",
        "\n",
        "transformers: Hugging Face's library for accessing pre-trained models.\n",
        "\n",
        "accelerate: Library to help speed up training and inference with large models.\n",
        "\n",
        "bitsandbytes: Library for quantization, reducing model size and potentially speeding up inference.\n",
        "\n",
        "huggingface_hub: Library for interacting with the Hugging Face Hub."
      ],
      "metadata": {
        "id": "O-fK4AvC-k1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-huggingface transformers accelerate bitsandbytes huggingface_hub"
      ],
      "metadata": {
        "id": "8_0c4XNMonKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the API key\n",
        "from google.colab import userdata\n",
        "key = userdata.get('HF_llm_rag')"
      ],
      "metadata": {
        "id": "9wRL3Ne3pM-x"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Set the Hugging Face API token as an environment variable for authentication.\n",
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = key"
      ],
      "metadata": {
        "id": "z1XycmZzrIUs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the HuggingFaceEndpoint class from langchain_huggingface to use models via the Inference API.\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# Initialize the HuggingFaceEndpoint with specified parameters.\n",
        "# repo_id: The ID of the model to use.\n",
        "# max_new_tokens: The maximum number of new tokens to generate in the response.\n",
        "# temperature: Controls the randomness of the output (higher means more random).\n",
        "# huggingfacehub_api_token: The API token for authentication.\n",
        "# task: The task the model is intended for (text generation in this case).\n",
        "\n",
        "llm = HuggingFaceEndpoint(repo_id=repo_id,\n",
        "                          max_new_tokens=256,\n",
        "                          temperature=0.7,\n",
        "                          huggingfacehub_api_token=key,\n",
        "                          task = 'text-genration')"
      ],
      "metadata": {
        "id": "leFUMNyprISU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke('What is machine learning?')"
      ],
      "metadata": {
        "id": "7jGP0C5lrINu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "\n",
        "repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# Configure BitsAndBytes quantization.\n",
        "# load_in_4bit: Load the model weights in 4-bit precision.\n",
        "# bnb_4bit_quant_type: Specify the quantization type (nf4 is a common type).\n",
        "# bnb_4bit_compute_dtype: The data type to use for computations (float16 is common).\n",
        "# bnb_4bit_use_double_quant: Whether to use double quantization.\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                   bnb_4bit_quant_type=\"nf4\",\n",
        "                   bnb_4bit_compute_dtype=\"float16\",\n",
        "                   bnb_4bit_use_double_quant=True)"
      ],
      "metadata": {
        "id": "u3uqW3ekrILU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize a HuggingFacePipeline from the model ID, enabling local loading and quantization.\n",
        "# model_id: The ID of the model to use.\n",
        "# model_kwargs: Additional arguments for loading the model, including the quantization config.\n",
        "# task: The task for the pipeline (text generation).\n",
        "# pipeline_kwargs: Arguments for the text generation pipeline.\n",
        "# max_new_tokens: Max tokens to generate.\n",
        "# do_sample: Whether to use sampling (False means greedy decoding).\n",
        "# repetition_penalty: Penalty for repeating tokens.\n",
        "# return_full_text: Whether to return the full text including the prompt.\n",
        "\n",
        "\n",
        "llm2 = HuggingFacePipeline.from_model_id(\n",
        "    model_id=repo_id,\n",
        "    model_kwargs={\"quantization_config\": quantization_config},\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 256,\n",
        "                     \"do_sample\":False,\n",
        "                      \"repetition_penalty\":1.03,\n",
        "                      \"return_full_text\":False,}\n",
        ")"
      ],
      "metadata": {
        "id": "nLiquJjS0kYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You're a helpful assistant\"),\n",
        "    HumanMessage(\n",
        "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "B6DVukYK3zbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ChatHuggingFace(llm=llm2, model_id=repo_id).invoke(messages)"
      ],
      "metadata": {
        "id": "7ak_tk7X1irD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yx2w1_vO3Tbb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}